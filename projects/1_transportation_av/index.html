<!DOCTYPE html> <html lang="en"> <head> <meta name="google-site-verification" content=""/><meta name="msvalidate.01" content=""/> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Transportation & Self-Driving | Yichao Cai 蔡逸超</title> <meta name="author" content="Yichao Cai 蔡逸超"/> <meta name="description" content="Perception projects in transportation and autonomous driving."/> <meta name="keywords" content="biorhythms, chronobiology, macroecology, shorebirds"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/logo.png"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://yichaocai1.github.io/projects/1_transportation_av/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Yichao Cai </span> 蔡逸超</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/repos/">repos</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h2 class="post-title">Transportation &amp; Self-Driving</h2> <p class="post-description">Perception projects in transportation and autonomous driving.</p> </header> <article> <p><strong>Precise Curb Detection for Autonomous Sanitation Vehicles</strong></p> <p>This project presents a novel framework for precise curb detection in autonomous sanitation vehicles, leveraging semantic segmentation with bird’s eye-view images for complex environments. Utilizing a lightweight HRNet for segmenting drivable areas and a zero-shot post-processing for curve fitting, alongside a modified RANSAC approach for outlier accommodation, the system demonstrates significant improvements in accuracy and robustness in challenging sanitation scenarios.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/projects/curb-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/projects/curb-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/projects/curb-1400.webp"></source> <img src="/assets/img/projects/curb.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="cor" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Curb detection visualizations in various scenarios. </div> <p><strong>Drivable Road Region Detection for Fixed-Route Autonomous Vehicles</strong></p> <p>This project proposes a drivable road region detection approach for fixed-route autonomous vehicles using computer vision and neural networks. By fusing images with local route maps into a Map-Fusion Image (MFI) and applying the FCN-VGG16 neural network model, this method enhances detection accuracy, especially in complex areas like intersections and turns without lane markings, and achieves greater robustness compared to traditional non-fused image methods.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/projects/autonomousdriving_cover-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/projects/autonomousdriving_cover-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/projects/autonomousdriving_cover-1400.webp"></source> <img src="/assets/img/projects/autonomousdriving_cover.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="cor" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Drivable road region detection based on a monocular camera. </div> <p><strong>Video-Based Driving Simulation Software for Traffic Sign Evaluation</strong></p> <p>A video driving simulation software has been developed for evaluating traffic signs. The process begins with the collection of video and vehicle movement data from actual driving scenarios. Subsequently, traffic signs within the collected footage are identified and tracked using image processing techniques. Newly designed traffic signs are then digitally inserted into the video, replacing their real-world counterparts. In the final step, vehicle movement data recorded during the drive is integrated into the video sequence, mirroring the motion experienced in the driving simulator. Participants interact with the simulation via the driving simulator’s throttle and brake pedals, allowing them to navigate through the video sequence. This control over playback speed and simulated movement provides a driving visualization and experience closely resembling that of real-world conditions.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/projects/drivingsimulation-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/projects/drivingsimulation-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/projects/drivingsimulation-1400.webp"></source> <img src="/assets/img/projects/drivingsimulation.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="cor" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Driving simulator with real-world scenarios. </div> <p><strong>Pedestrian Head Pose Estimation Using Surveillance Video</strong></p> <p>This project contributes to human-vehicle interaction research in autonomous driving by analyzing pedestrian movement and head orientation through surveillance video of road crossings. It encompasses key vision tasks such as pedestrian and head detection, head pose estimation, multi-object tracking, and head position determination. Utilizing a novel dataset created from 10 participants with varying appearances, and employing advanced techniques like CNN-based classification and a combination of detection models with DeepSort, the project aims at improving the precision of pedestrian behavior analysis in autonomous driving contexts.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/projects/headpose-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/projects/headpose-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/projects/headpose-1400.webp"></source> <img src="/assets/img/projects/headpose.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="cor" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Overall tasks of pedestrian head pose estimation. </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Yichao Cai 蔡逸超. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Last updated: June 12, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script type="text/javascript">$(function(){$('[data-toggle="tooltip"]').tooltip()});</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> </body> </html>