<!DOCTYPE html> <html lang="en"> <head> <meta name="google-site-verification" content=""/><meta name="msvalidate.01" content=""/> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Thoughts on Contrastive Representation Learning | Yichao Cai 蔡逸超</title> <meta name="author" content="Yichao Cai 蔡逸超"/> <meta name="description" content="Reflections after reading papers on contrastive learning theory."/> <meta name="keywords" content="biorhythms, chronobiology, macroecology, shorebirds"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/logo.png"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://yichaocai1.github.io/blog/2024/contrastive-learning/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Yichao Cai </span> 蔡逸超</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/repos/">repos</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Thoughts on Contrastive Representation Learning</h1> <p class="post-meta">January 17, 2024</p> <p class="post-tags"> <a href="/blog/2024"> <i class="fas fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/study"> <i class="fas fa-hashtag fa-sm"></i> study</a>     ·   <a href="/blog/category/thoughts"> <i class="fas fa-tag fa-sm"></i> thoughts</a>   </p> </header> <article class="post-content"> <blockquote> <p>This passage initiates by addressing the question depicted in <span style="color:purple">fig.1 text</span>. Despite reviewing the referenced papers <span style="color:purple">[1-6]</span>, I acknowledge lingering uncertainties regarding the nuances of contrastive representation learning. Therefore, I draft this for later review and reflections.</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/cl_pair1-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/cl_pair1-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/cl_pair1-1400.webp"></source> <img src="/assets/img/posts/cl_pair1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 1. Are these two samples (either in text-modality and image-modality) are appropriate to be considered a positive pair as the learning objective for disentangling the true content variable (Images generated with SDv2.1) </div> <p>Recent approaches utilize data augmentations as weak supervision to distinguish retained information, termed ”content”, from discarded information, termed “style” <span style="color:purple">[2]</span>. However, the agreement on real-world data generation process, particularly the identifiability of true causal relations in data generative mechanisms, currently lacks consensus.This lack of agreement introduces uncertainty around terminologies like ”content” and ”style”, and their distinct definitions hinge on how we conceptualize them and how we set the learning objective. Nevertheless, this issue seems to be primarily a matter of nomenclature. The resolution of this uncertainty is anticipated to come with consensus on the identifiability of true mechanisms of data generative process.</p> <p>Although current researches demonstrates that the content variable causes the style variables, a question arises to me regarding the interpretation of ”content”. Specifically, as shown in <span style="color:purple">fig.2</span>, does content” in these works refer to (a) the latent variable governing the class name of a sample, causing other latent variables governing properties like color, position, background, etc., or (b) an intrinsic latent variable that is a common cause of other properties, including the class name?The distinction between these two interpretations is substantial, and it appears to be a pivotal for understanding disentaglement by contrastive representation learning – The literature says that representations of two positve samples should be mapped to nearby features, and thus be mostly invariant to not needed noise factors <span style="color:purple">[4]</span>. However, from a causal perspective, I posit that the invariance to specific noise factors (i.e., latent variables) is not determined by whether the noise factors are ”needed” for a task, but rather by the underlying causal relationship between latent variables, which constrains how one constructs the learning objective.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/image_3-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/image_3-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/image_3-1400.webp"></source> <img src="/assets/img/posts/image_3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/image_2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/image_2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/image_2-1400.webp"></source> <img src="/assets/img/posts/image_2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 2. Which generative process is more appropriate? In the left, model (a), the latent variable governing class name of is considered the content variable, which act as the common cause of other properties of a sample; while model (b) indicates a higher level of content. </div> <p>Now, let’s consider the two assumed situations: If the former data-generation process (a) is correct, for a classification task, one would only need to impose changes to retain class-level information, as other factors are considered noise for this task. In this scenario, positive pairs would simply need to share the same class name, while data with different class names should be considered negative counterparts. However, the effectiveness of such representations in achieving supreme robustness and accuracy in classification tasks may be limited. While this approach might work well for images containing only one class of large-scaled objects, as seen in synthetic datasets like 3DIdent and its variations, it may not be suitable for real-world images with complex backgrounds, multiple objects of different classes, and sometimes small-scaled target objects. Looking at supervised classification methods, although they do not follow a contrastive-learning protocol, their learning objectives also emphasize class-level information by treating samples with the same class name as positive samples. However, these methods often face out-of-distribution (OOD) issues, suggesting that the generative process (a) may not be appropriate, and positive pairs sharing the same class name might not be sufficient to obtain disentangled representations, regardless of the changes imposed on the data.</p> <blockquote> <p><strong>Discussion 1.</strong> The varied versions of 3DIdent datasets (i.e., 3DIdent, Causal3DIdent, Multimodal3DIdent) may be insufficient to verify if the intrinsic content variable is disentangled, as they lack inner-class variations of objects. In these datasets, all the samples sharing the same class name should be considered as the same object, as long as we one agree that traditional augmentation techniques (color distortion, rotation, crop, etc.) does not alter the object identity. Yet, defining ”the same object” in data is not straightforward, as some works regard different views augmented from a same sample by ”texture randomization” as a positive pair <span style="color:purple">[5]</span> – This perspective contrasts with the intuition that topology and shape determine a coarse-grained class of objects, while additional texture information defines object identity (or a fine-grained class of objects).</p> </blockquote> <p>On the contrary, if the latter data-generation process (b) is more appropriate, changes need to be imposed on data without altering the fact that the target objects in a positive pair should be the same instances, i.e., maintaining object identity. In this context, the answer to the question illustrated in fig. 1 would be that two images with different bicycles should not be considered a positive pair, nor should the two text prompts. This perspective explains this disconcert: considering a positive pair like ”a dog in a sketch” and ”a photo of a yellow dog,” this prompt pair cannot constrain the dog as the same identity (similar shape) in the two prompts. Consequently, only to disentangle the variable governing the class name by aligning the samples with the same class names is unachievable due to the unresolved dependence on \(\mathbf{z}_{class}\) variable and other latent variables without modeling the true content variable.</p> <p>Xiao et al., 2020 <span style="color:purple">[5]</span> argue that current methods introduce inductive bias by encouraging neural networks to be less sensitive to information regarding augmentation, which may help or hurt. However, I believe that this “hurt” is attributed to the insufficient disentanglement of the latent content variable due to limited changes. Upon the (asymptotical) disentanglement of the intrinsic content, the resulting representations should distributed on hypersphere \(\mathcal{S}^{n-1}\) in a n-dimensional space according to the common assumption of data manifold in contrastive learning literature. Therefore, the representations could be applied to instance identification with clustering by spherical distance on this hypersphere, and with the linear separablility of the representations, other downstream tasks (relating to one or several properties of a sample, such as classification, action detection, etc.) can be realized with linear combinations of disentangled representations. This can be regarded as the projection of hypersphere along one or several bases/axis to a lower-dimensional hypersphere (\(\mathcal{S}^{n-m}\)).</p> <p>To clarify the goal of disentangled representation learning, it becomes imperative to ensure that samples of a positive pair share the same indentity, and enough changes are imposed between the negative counterparts. Yet, current methods in contrastive representation learning literature faces challenges on one or both of the two aspects for real-world data:</p> <ul> <li>Different augmented views of an image (e.g., SimCLR, BYOL, etc.) are considered suitable positive pairs for isolating the content. However, the combination of the traditional data augmentation methods are not sufficient to impose enough changes on data to cast aside all the style information.</li> <li>Employing only augmented text pairs makes it easy to impose (some aspects of) style changes on data due to the semantic and logical nature of text.However, class names in a prompt provide only the class-level constraint on positive samples, making it challenging to determine the identity of an instance solely using text data.</li> <li>Contrastive language-image training (e.g., CLIP) uses Image-text pairs, where the both parts of a image-text pair can be considered two different views of one sample, if the caption of the image is detailed, e.g. in <span style="color:purple">fig. 3</span>. However, using only image-text pairs may not be capable for disentangling the intrinsic content, as text data lacks the informativeness needed to precisely constrain the sample identity in its imagery counterpart. For instance, even if one adds numerous attributive adjectives to an object in the textual modality (e.g. “dog”), the text cannot be constrained to represent only the exact same dog as shown in the image.</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/oldman1-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/oldman1-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/oldman1-1400.webp"></source> <img src="/assets/img/posts/oldman1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 3. An old American person stand in front of a house with white wall, with his black dog by his side. (Images generated with SDv2.1) </div> <p>Therefore, there might be a need to develop a method that combines the logic and semantic nature of textual modality with the informative nature of image modality. Text data is inherently more recaptitulative (in a property-wise manner) than image data, while image data is more precise than text data in describing “the exact same object(s)/event(s)” due to its greater informativeness than text data (per image vs.per text prompt, not per memory byte).</p> <p><strong>Reference</strong></p> <p>[1] Daunhawer, I., Bizeul, A., Palumbo, E., Marx, A., and Vogt, J. E. (2022). Identifiability results for multimodal contrastive learning. In The Eleventh International Conference on Learning Representations.</p> <p>[2] Eastwood, C., von K ̈ugelgen, J., Ericsson, L., Bouchacourt, D., Vincent, P., Ibrahim, M., and Sch ̈olkopf, B. (2023). Self-supervised disentanglement by leveraging structure in data augmentations. In Causal Representation Learning Workshop at NeurIPS 2023.</p> <p>[3] Von K ̈ugelgen, J., Sharma, Y., Gresele, L., Brendel, W., Sch ̈olkopf, B., Besserve, M., and Locatello, F. (2021). Self-supervised learning with data augmentations provably isolates content from style. Advances in neural information processing systems, 34:16451–16467.</p> <p>[4] Wang, T. and Isola, P. (2020). Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In International Conference on Machine Learning, pages 9929–9939. PMLR.</p> <p>[5] Xiao, T., Wang, X., Efros, A. A., and Darrell, T. (2020). What should not be contrastive in contrastive learning. In International Conference on Learning Representations.</p> <p>[6] Zimmermann, R. S., Sharma, Y., Schneider, S., Bethge, M., and Brendel, W. (2021). Contrastive learning inverts the data generating process. In International Conference on Machine Learning, pages 12979 12990. PMLR.</p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Yichao Cai 蔡逸超. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Last updated: June 12, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script type="text/javascript">$(function(){$('[data-toggle="tooltip"]').tooltip()});</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> </body> </html>