<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://yichaocai1.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://yichaocai1.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-06-12T10:52:06+00:00</updated><id>https://yichaocai1.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Three Weekly Self-Introspections</title><link href="https://yichaocai1.github.io/blog/2024/self-introspections/" rel="alternate" type="text/html" title="Three Weekly Self-Introspections"/><published>2024-05-25T18:30:00+00:00</published><updated>2024-05-25T18:30:00+00:00</updated><id>https://yichaocai1.github.io/blog/2024/self-introspections</id><content type="html" xml:base="https://yichaocai1.github.io/blog/2024/self-introspections/"><![CDATA[<p>If possible, every <em>Monday evening</em>, ask myself these questions, reflect, and think before moving on to next week’s work:</p> <ol> <li><strong>Did I take the time to learn new knowledge beyond my daily tasks this week?</strong> <ul> <li>Engaging in activities that expanded my understanding beyond immediate research needs. This could include reading new papers, attending seminars, or exploring different fields that could provide a fresh perspective.</li> </ul> </li> <li><strong>Did my research make tangible progress this week?</strong> <ul> <li>Assessing the specific advancements made in my research, including sucessful/failure experiments, writing, or documenting new ideas.</li> </ul> </li> <li><strong>Is my research still guided by my interest and vision?</strong> <ul> <li>Ensuring that the work remains aligned with my core interests and long-term vision. Keeping the passion alive!</li> </ul> </li> </ol>]]></content><author><name></name></author><category term="study"/><summary type="html"><![CDATA[To help maintain focus and progress in my research.]]></summary></entry><entry><title type="html">How Information Bottleneck Helps Representation Learning</title><link href="https://yichaocai1.github.io/blog/2024/information-bottleneck/" rel="alternate" type="text/html" title="How Information Bottleneck Helps Representation Learning"/><published>2024-04-16T20:30:00+00:00</published><updated>2024-04-16T20:30:00+00:00</updated><id>https://yichaocai1.github.io/blog/2024/information-bottleneck</id><content type="html" xml:base="https://yichaocai1.github.io/blog/2024/information-bottleneck/"><![CDATA[<p>This blog is on the topic of Information Bottleneck (IB) principle, clarifying its connection with rate-distortion theory, exploring different optimization strategies for IB, and discussing the application of the Variational Information Bottleneck (VIB) and (\beta)-VAE within the context of representation learning. Designed as both a personal reference and a resource for others, this blog aims to provide a holistic view understanding of the IB principle.</p> <p><strong>Please refer to the document available through the following link: <a href="/assets/pdf/IB.pdf">Information Bottleneck</a>.</strong></p> <p><span style="color:red"> (The blog is currently incomplete and will be updated periodically as I continue to expand my knowledge and understanding of the topic.) </span></p>]]></content><author><name></name></author><category term="reading"/><category term="study"/><summary type="html"><![CDATA[From the Rate Distortion Theory to β−VAE]]></summary></entry><entry><title type="html">Thoughts on Contrastive Representation Learning</title><link href="https://yichaocai1.github.io/blog/2024/contrastive-learning/" rel="alternate" type="text/html" title="Thoughts on Contrastive Representation Learning"/><published>2024-01-17T05:00:00+00:00</published><updated>2024-01-17T05:00:00+00:00</updated><id>https://yichaocai1.github.io/blog/2024/contrastive-learning</id><content type="html" xml:base="https://yichaocai1.github.io/blog/2024/contrastive-learning/"><![CDATA[<blockquote> <p>This passage initiates by addressing the question depicted in <span style="color:purple">fig.1 text</span>. Despite reviewing the referenced papers <span style="color:purple">[1-6]</span>, I acknowledge lingering uncertainties regarding the nuances of contrastive representation learning. Therefore, I draft this for later review and reflections.</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/cl_pair1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/cl_pair1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/cl_pair1-1400.webp"/> <img src="/assets/img/posts/cl_pair1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1. Are these two samples (either in text-modality and image-modality) are appropriate to be considered a positive pair as the learning objective for disentangling the true content variable (Images generated with SDv2.1) </div> <p>Recent approaches utilize data augmentations as weak supervision to distinguish retained information, termed ”content”, from discarded information, termed “style” <span style="color:purple">[2]</span>. However, the agreement on real-world data generation process, particularly the identifiability of true causal relations in data generative mechanisms, currently lacks consensus.This lack of agreement introduces uncertainty around terminologies like ”content” and ”style”, and their distinct definitions hinge on how we conceptualize them and how we set the learning objective. Nevertheless, this issue seems to be primarily a matter of nomenclature. The resolution of this uncertainty is anticipated to come with consensus on the identifiability of true mechanisms of data generative process.</p> <p>Although current researches demonstrates that the content variable causes the style variables, a question arises to me regarding the interpretation of ”content”. Specifically, as shown in <span style="color:purple">fig.2</span>, does content” in these works refer to (a) the latent variable governing the class name of a sample, causing other latent variables governing properties like color, position, background, etc., or (b) an intrinsic latent variable that is a common cause of other properties, including the class name?The distinction between these two interpretations is substantial, and it appears to be a pivotal for understanding disentaglement by contrastive representation learning – The literature says that representations of two positve samples should be mapped to nearby features, and thus be mostly invariant to not needed noise factors <span style="color:purple">[4]</span>. However, from a causal perspective, I posit that the invariance to specific noise factors (i.e., latent variables) is not determined by whether the noise factors are ”needed” for a task, but rather by the underlying causal relationship between latent variables, which constrains how one constructs the learning objective.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/image_3-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/image_3-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/image_3-1400.webp"/> <img src="/assets/img/posts/image_3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/image_2-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/image_2-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/image_2-1400.webp"/> <img src="/assets/img/posts/image_2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 2. Which generative process is more appropriate? In the left, model (a), the latent variable governing class name of is considered the content variable, which act as the common cause of other properties of a sample; while model (b) indicates a higher level of content. </div> <p>Now, let’s consider the two assumed situations: If the former data-generation process (a) is correct, for a classification task, one would only need to impose changes to retain class-level information, as other factors are considered noise for this task. In this scenario, positive pairs would simply need to share the same class name, while data with different class names should be considered negative counterparts. However, the effectiveness of such representations in achieving supreme robustness and accuracy in classification tasks may be limited. While this approach might work well for images containing only one class of large-scaled objects, as seen in synthetic datasets like 3DIdent and its variations, it may not be suitable for real-world images with complex backgrounds, multiple objects of different classes, and sometimes small-scaled target objects. Looking at supervised classification methods, although they do not follow a contrastive-learning protocol, their learning objectives also emphasize class-level information by treating samples with the same class name as positive samples. However, these methods often face out-of-distribution (OOD) issues, suggesting that the generative process (a) may not be appropriate, and positive pairs sharing the same class name might not be sufficient to obtain disentangled representations, regardless of the changes imposed on the data.</p> <blockquote> <p><strong>Discussion 1.</strong> The varied versions of 3DIdent datasets (i.e., 3DIdent, Causal3DIdent, Multimodal3DIdent) may be insufficient to verify if the intrinsic content variable is disentangled, as they lack inner-class variations of objects. In these datasets, all the samples sharing the same class name should be considered as the same object, as long as we one agree that traditional augmentation techniques (color distortion, rotation, crop, etc.) does not alter the object identity. Yet, defining ”the same object” in data is not straightforward, as some works regard different views augmented from a same sample by ”texture randomization” as a positive pair <span style="color:purple">[5]</span> – This perspective contrasts with the intuition that topology and shape determine a coarse-grained class of objects, while additional texture information defines object identity (or a fine-grained class of objects).</p> </blockquote> <p>On the contrary, if the latter data-generation process (b) is more appropriate, changes need to be imposed on data without altering the fact that the target objects in a positive pair should be the same instances, i.e., maintaining object identity. In this context, the answer to the question illustrated in fig. 1 would be that two images with different bicycles should not be considered a positive pair, nor should the two text prompts. This perspective explains this disconcert: considering a positive pair like ”a dog in a sketch” and ”a photo of a yellow dog,” this prompt pair cannot constrain the dog as the same identity (similar shape) in the two prompts. Consequently, only to disentangle the variable governing the class name by aligning the samples with the same class names is unachievable due to the unresolved dependence on \(\mathbf{z}_{class}\) variable and other latent variables without modeling the true content variable.</p> <p>Xiao et al., 2020 <span style="color:purple">[5]</span> argue that current methods introduce inductive bias by encouraging neural networks to be less sensitive to information regarding augmentation, which may help or hurt. However, I believe that this “hurt” is attributed to the insufficient disentanglement of the latent content variable due to limited changes. Upon the (asymptotical) disentanglement of the intrinsic content, the resulting representations should distributed on hypersphere \(\mathcal{S}^{n-1}\) in a n-dimensional space according to the common assumption of data manifold in contrastive learning literature. Therefore, the representations could be applied to instance identification with clustering by spherical distance on this hypersphere, and with the linear separablility of the representations, other downstream tasks (relating to one or several properties of a sample, such as classification, action detection, etc.) can be realized with linear combinations of disentangled representations. This can be regarded as the projection of hypersphere along one or several bases/axis to a lower-dimensional hypersphere (\(\mathcal{S}^{n-m}\)).</p> <p>To clarify the goal of disentangled representation learning, it becomes imperative to ensure that samples of a positive pair share the same indentity, and enough changes are imposed between the negative counterparts. Yet, current methods in contrastive representation learning literature faces challenges on one or both of the two aspects for real-world data:</p> <ul> <li>Different augmented views of an image (e.g., SimCLR, BYOL, etc.) are considered suitable positive pairs for isolating the content. However, the combination of the traditional data augmentation methods are not sufficient to impose enough changes on data to cast aside all the style information.</li> <li>Employing only augmented text pairs makes it easy to impose (some aspects of) style changes on data due to the semantic and logical nature of text.However, class names in a prompt provide only the class-level constraint on positive samples, making it challenging to determine the identity of an instance solely using text data.</li> <li>Contrastive language-image training (e.g., CLIP) uses Image-text pairs, where the both parts of a image-text pair can be considered two different views of one sample, if the caption of the image is detailed, e.g. in <span style="color:purple">fig. 3</span>. However, using only image-text pairs may not be capable for disentangling the intrinsic content, as text data lacks the informativeness needed to precisely constrain the sample identity in its imagery counterpart. For instance, even if one adds numerous attributive adjectives to an object in the textual modality (e.g. “dog”), the text cannot be constrained to represent only the exact same dog as shown in the image.</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/oldman1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/oldman1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/oldman1-1400.webp"/> <img src="/assets/img/posts/oldman1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 3. An old American person stand in front of a house with white wall, with his black dog by his side. (Images generated with SDv2.1) </div> <p>Therefore, there might be a need to develop a method that combines the logic and semantic nature of textual modality with the informative nature of image modality. Text data is inherently more recaptitulative (in a property-wise manner) than image data, while image data is more precise than text data in describing “the exact same object(s)/event(s)” due to its greater informativeness than text data (per image vs.per text prompt, not per memory byte).</p> <p><strong>Reference</strong></p> <p>[1] Daunhawer, I., Bizeul, A., Palumbo, E., Marx, A., and Vogt, J. E. (2022). Identifiability results for multimodal contrastive learning. In The Eleventh International Conference on Learning Representations.</p> <p>[2] Eastwood, C., von K ̈ugelgen, J., Ericsson, L., Bouchacourt, D., Vincent, P., Ibrahim, M., and Sch ̈olkopf, B. (2023). Self-supervised disentanglement by leveraging structure in data augmentations. In Causal Representation Learning Workshop at NeurIPS 2023.</p> <p>[3] Von K ̈ugelgen, J., Sharma, Y., Gresele, L., Brendel, W., Sch ̈olkopf, B., Besserve, M., and Locatello, F. (2021). Self-supervised learning with data augmentations provably isolates content from style. Advances in neural information processing systems, 34:16451–16467.</p> <p>[4] Wang, T. and Isola, P. (2020). Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In International Conference on Machine Learning, pages 9929–9939. PMLR.</p> <p>[5] Xiao, T., Wang, X., Efros, A. A., and Darrell, T. (2020). What should not be contrastive in contrastive learning. In International Conference on Learning Representations.</p> <p>[6] Zimmermann, R. S., Sharma, Y., Schneider, S., Bethge, M., and Brendel, W. (2021). Contrastive learning inverts the data generating process. In International Conference on Machine Learning, pages 12979 12990. PMLR.</p>]]></content><author><name></name></author><category term="thoughts"/><category term="study"/><summary type="html"><![CDATA[Reflections after reading papers on contrastive learning theory.]]></summary></entry><entry><title type="html">Reading Report of Causal Inference</title><link href="https://yichaocai1.github.io/blog/2021/causal-inference/" rel="alternate" type="text/html" title="Reading Report of Causal Inference"/><published>2021-11-22T23:10:00+00:00</published><updated>2021-11-22T23:10:00+00:00</updated><id>https://yichaocai1.github.io/blog/2021/causal-inference</id><content type="html" xml:base="https://yichaocai1.github.io/blog/2021/causal-inference/"><![CDATA[<blockquote> <p>“Correlation does not imply causation.”</p> </blockquote> <p>Given an input variable \(X\), an outcome variable \(Y\) and observations \({(x_i, y_i)}\), probabilistic models can find a correlation between these variables \(X\) and \(Y\), as well as can give a prediction of \(Y\) after observing \(X = x\). One critical hypothesis of probabilistic models is observations \({(x_i, y_i)}\) are realizations of random variables \({(X_i, Y_i)}\) that are independently and identically distributed (i.i.d.) with joint distribution \(P_{X,Y}\). However, such postulation can be violated and statistically observed correlations are not always reliable, thus we need to build a generic relationship between an effect and the cause that gives rise to it, namely causality.</p> <p><strong>Structural causal model</strong></p> <p>To this end, the structural causal model (SCM) is a methodology to well describe causality, which not only entails a joint distribution over all observables (like purely probabilistic descriptions), but also incorporates structural assignments about how \(P_{X,Y}\) come about (directional relationships between variables) and the effect of interventions on variables. We can use an SCM modeling a system in an observational state and under perturbations at the same time. It is even possible to regard SCMs as models for counterfactual statements.</p> <p>Before introducing SCMs, it is worth mentioning the principle of independent causal mechanisms (ICMs). To my understanding, it implies (1) one can intervene on one mechanism without affecting other mechanisms (“ independence of mechanisms”), (2) the mechanism that generates the effect from its cause contains no information about the mechanism generating the cause (“ independence of cause and mechanism”) and (3) the noises of variables are independent (“ independence of noises”). We shall see that SCMs submit to the assumptions in the remainder of this report.</p> <p>Formally, an bivariate SCM with graph \(C\rightarrow E\) consists of two assignments:</p> \[C:=N_C, ~~E:=f_E(C, N_E),~~ where~~ N_E \perp\!\!\!\perp N_C. \tag{1}\] <p>In this model, random variable \(C\) is the cause and \(E\) the effect. The “\(:=\)” symbol is so-called a structural assignment which implies a directional causal effect besides the assignment function. More generally, a multivariate SCM \(\boldsymbol{\zeta}\) consists of a collection \(\mathbf{S}\) of \(d\) structural assignments:</p> \[X_j := f_j(PA_j, N_j), ~~~~ j=1, ~..., ~d, \tag{2}\] <p>where \(d\) is the number of variables; \(PA_j\) are the parents of \(X_j\), also known as direct causes; and the noises \(N_j (j = 1, 2, ..., d)\) are required to be jointly independent (coinciding with “independence of noises”). An SCM \(\boldsymbol{\zeta}\) entails a unique distribution over the variables \(\mathbf{X}\) which is called entailed distribution \(P_\mathbf{X}^{\boldsymbol{\zeta}}\). Structural assignments are a set of assignments or functions that tell us how certain variables determine others. Each SCM has a corresponding directional acyclic graph (DAG), where vertices represent the variables and edges the causal direction between two variables.</p> <p>Now that we are talking about DAGs, Markov property is a non-negligible term. Assume a distribution is Markovian: The global Markov property says one can determine \(\mathbf{A}\) is independent to \(\mathbf{B}\) given \(\mathbf{C}\) if the path between vertices set \(\mathbf{A}\) and \(\mathbf{B}\) are d-separated by the vertices set \(\mathbf{C}\), which is equivalent to the local Markov property such that each variable is independent of its non-descendants given its parents. Based on the Markov property, one can say \(X\) and \(Y\) are observational dependent if there is any unblocked path (the path can not contain a collider) between these two variables.</p> <p>Although having graphs and Markov condition, SCMs are primarily worked with for causal inference because they contain their corresponding DAGs and imply Markov property (that is, \(P_\mathbf{X}\) entailed by an SCM is Markovian with respect to the corresponding DAG \(\mathfrak{g}\)). Maybe more significantly, restricting the function class in SCMs can lead to identifiability of the causal structure.</p> <p><strong>Identifiability</strong></p> <p>Identifiability means the causal relationship between variables, or the causal structure of a multivariate model. We do structural identification to identify true causal directions, that is, to determine a reasonable SCM from statistical observations. However, the joint distribution \(P_{X,Y}\) does not tell us whether it has been induced by the SCM from \(X\) to \(Y\) or the other one from \(Y\) to \(X\). Since the same observational distribution can be generated by different SCMs, the causal structure is not identifiable from only the joint distribution. Generally speaking, the causal direction between just two observed variables cannot be inferred from passive observations alone, which means the hypothetical causal direction \(X\rightarrow Y\) can not be determined if we do not impose any constraint on the function \(\mathbf{f}: \mathcal{X\rightarrow Y}\). Thus, we have to render certain constraints on the function so that the independence of cause holds for only one direction, that is, the identifiability of the causal direction. There are several appropriately defined model classes and the identifiability of their causal directions.</p> <p>Let us first go from a bivariate linear model: Assume a linear model admitted by \(P_{X,Y}\),</p> \[Y = \alpha X + N_Y, ~N_Y \perp\!\!\!\perp X. \tag{3}\] <p>There exists \(\beta \in R\) and a random variable \(N_X\) validate a backward model such that</p> \[X = \beta Y + N_X, ~N_X \perp\!\!\!\perp Y, \tag{4}\] <p>if and only if \(N_Y\) and \(X\) are all Gaussian (due to the symmetricity of Gaussian distribution). In other words, one can rigorously say that the causal direction of a linear model is identifiable if at most one of \(X\) and \(N_Y\) is Gaussian distributed, which is also known as a Linear non-Gaussian acyclic model (LiNGAM). Nonlinear additive noise models (ANMs) are a more generic model class than LiNGAMs, because nonlinear transformation is often involved in practice. An ANM is defined as</p> \[Y = f_Y(X) + N_Y, ~~N_Y \perp\!\!\!\perp X, \tag{5}\] <p>where \(f_Y\) is a measurable function (Lebesgue integrable). It has shown that the set of densities \(p_X\) for which the obtained joint distribution \(P_{X,Y}\) admits a backward ANM of \(Y \rightarrow X\) is contained in a 3-dimensional affine space. Since the space of all possible \(P_X\) is infinite dimensional, the three-dimensional sub-space is rather a complicated condition (To linear model, one can think the condition is that \(N_Y\) and \(X\) are all Gaussian). Therefore, we could roughly think, in the “generic” case, the causal direction of an ANM model is identifiable. Moreover, there is another model class so-called post-nonlinear models of which the causal direction has been shown identifiable except for some rare non-generic cases.</p> <p>In terms of multivariate SCMs, more generally, the Markov equivalence class of \(\mathfrak{g}^0\) is identifiable from \(P_\mathbf{X}\) under satisfying the Markov condition and faithfulness simultaneously such that</p> \[\mathbf{A} \perp\!\!\!\perp_{\mathfrak{g}^0} \mathbf{B} ~\lvert ~\mathbf{C} \Rightarrow \mathbf{A} \perp\!\!\!\perp \mathbf{B} ~\lvert ~\mathbf{C} ~~~ and ~~~ \mathbf{A} \perp\!\!\!\perp_{\mathfrak{g}^0} \mathbf{B} ~\lvert ~\mathbf{C} \Leftarrow \mathbf{A} \perp\!\!\!\perp \mathbf{B} ~\lvert ~\mathbf{C} \tag{6}\] <p>for all disjoint vertices sets \(\mathbf{A, B}\) and \(\mathbf{C}\). Furthermore, each graph in \(\mathrm{CPGAD}( \mathfrak{g}^0)\) entails the same joint distribution \(P_\mathbf{X}\); for any graph \(\mathfrak{g}\) which is not in the Markov equivalence class of \(\mathfrak{g}^0\), the entailed joint distribution \(P_\mathbf{X}^\mathfrak{g}\) is not simultaneously Markovian and faithful. The structural identifiability spontaneously implies causal minimality since faithfulness is more rigorous than causal minimality. For ANMs, structural identifiability means each function \(f_j\) for node \(j\) is not constant in any of its arguments. Specifically, we further have linear Gaussian models with equal error variances, LiNGAMs and nonlinear Gaussian ANMs are structurally identifiable. Without having a priori knowledge of the mechanism, it is more appropriate to assume the SCM to be general (but not too flexible to loose the identifiability) since too restrictive and idealized assumptions may lead to misunderstanding of the true causality.</p> <p>There are mainly two strategies for structure identification, namely independence-based methods and score-based methods. Most independece-based methods contain two stages: (1) estimate the undirected edges, or say the skeleton; (2) orient as many edges as possible afterward (for ANMs, one can test the independence of residuals or use a maximum-likelihood approach). On the other hand, the score-based strategy is to assign a score to each possible graph and search over the space of DAGs to find the graph with the highest score. For example, there are supervised learning methods to consider causal inference as a classification task in machine learning. It is recommended in the book that these methods can make efficient use of known identifiability properties or combinations of them to become more useful in practice.</p> <p><strong>Intervention</strong> Roughly speaking, an intervention is to set a variable (or several variables) in an SCM to an invariant or specific distribution without changing other mechanisms (coinciding with the principle of independent mechanisms), and it is a powerful tool to reason out causal relationships. Different from passively observing a variable \(X = x\), an intervention is to set \(X := x\). After an intervention, a variable is no more influenced by its parents in the original SCM. For example, if we replace the assignment in Formula (1) by \(E := 4\), this intervention is denoted by \(do(E:=4)\). Taxonomically, to set a variable to a fixed value is called hard intervention (the DAG is correspondingly modified by removing all incoming arrows to the intervened variable), while a soft intervention is to alter the noise distribution of \(E\) and keep a functional dependence on \(C\).</p> <p>In a bivariate SCM \(C \rightarrow E\), <strong>(i)</strong> an intervention on the cause \(C\) will change the distribution of the cause \(E\), but <strong>(ii)</strong> no matter how strongly we intervene on the effect, the distribution of the cause remains what it was before; and importantly, <strong>(iii)</strong> the conditional distribution of \(C\) given \(E=e\) is different from the distribution of \(C\) after setting \(do(E:=e)\). For example, suppose an SCM</p> \[C:=N_C, ~~E := 4\cdot C + N_E, ~~ where ~ N_C, ~~N_E \mathop{\sim}^{iid} \mathcal{N}(0,1), \tag{7}\] <p>\(P_E = \mathcal{N}(0,17)\) is not equal to \(P_E^{do(C:=1)} = \mathcal{N}(0,17)\) nor \(P_E^{do(C:=4)} = \mathcal{N}(16,1)\), which numerically attests to statement <strong>(i)</strong>; \(P_C = P^{do(E:=4)}_C = P^{do(E:=2.2)}_C = \mathcal{N}(0, 1)\), which illustrates statement <strong>(ii)</strong>; \(P^{do(E:=2)}_C\) is not equal to \(P_{C\lvert E = 2}\), which illustrates statement <strong>(iii)</strong>.</p> <p>More generally, for a multivariate SCM \(\boldsymbol{\zeta}\), an intervention means to replace one of the structural assignments to obtain a new SCM \(\tilde{\boldsymbol{\zeta}}\). Thus, the entailed distribution \(P_\mathbf{X}^{\boldsymbol{\zeta}}\) is changed to a new distribution, namely the intervention distribution. If the structural assignment \(X_k\) is intervened on by</p> \[X_k := \tilde{f}(\tilde{\bf PA}_k, \tilde{N}_k), \tag{8}\] <p>then the corresponding intervention distribution is</p> \[P_{\bf X}^{\tilde{\boldsymbol{\zeta}}} := P_{\bf X}^{\boldsymbol{\zeta};do(X_k:=\tilde{f}(\tilde{\bf PA}_k, \tilde{N_k}))}. \tag {9}\] <p>Intervention distributions differ from the observation distribution. So, how to estimate intervention distributions? For any SCMs, one can compute an intervention distribution from observational quantities such that</p> \[p^{\tilde{\boldsymbol{\zeta}}; do({X_k := \tilde N_k)}}(x_1, ..., x_d) = \tilde p(\tilde N_k)\prod\nolimits_{j\not=k} p^{\boldsymbol{\zeta}} (x_j\lvert x_{pa(j)}). \tag {10}\] <p>for any SCM \(\tilde{\boldsymbol{\zeta}}\) that is constructed from \(\boldsymbol{\zeta}\) by intervening on \(X_k\) but not on \(X_j\) (or some variables excluding \(X_j\)).</p> <p>One can also compute an interventional distribution \(p^{\boldsymbol{\zeta};do(X:=x)}(y)\), when there is a valid adjustment set \(\bf Z\) of \({X, Y}\) (through “parent adjustment”, “backdoor criterion”, “toward necessity”) such that</p> \[p^{\tilde{\boldsymbol{\zeta}};do({X := x})}(y) = \sum\nolimits_{\bf z}p^{\boldsymbol{\zeta}}(y\lvert x, {\bf z})p^{\boldsymbol{\zeta}}({\bf z}). \tag {12}\] <p>Sometimes, if the value of \(X\) does not depend on \(\mathbf{Z}\) directly but only through a propensity score \(L:=Ll({\bf Z})\), which means \(X\) and \({\bf Z}\) are independent given \(L(\mathbf{Z})\), one can also compute \(p^{\boldsymbol{\zeta};do(X:=x)}(y)\) analogously to <span style="color:purple">eq.(12)</span> by replacing \(\mathbf{Z}\) to \(L(\mathbf{Z})\). It is said in <span style="color:purple">[1]</span> that the later method may lead to a better estimation of an interventional distribution: Although one needs to estimate the function \(L\), the result conditional \(p^{\boldsymbol{\zeta}}(y\lvert l,x)\) is potentially lower dimensional than \(p^{\boldsymbol{\zeta}}(y\lvert z,x)\).</p> <p>Interventional distributions can be estimated consistently from randomized experiments. However, two different SCMs can have the same interventional distributions, namely interventionally equivalent, if they have the same interventional distributions on all single nodes interventions. Two interventionally equivalent SCMs are not always counterfactually equivalent.</p> <p><strong>Conterfactual resoning</strong></p> <p>We often think in counterfactuals in our everyday life — “I should have unloaded these shares yesterday.”, “I could not have been late, had I caught the bus.”, and so on. There is no way that we could have possibly known the outcome before it happened, however, such counterfactual statements do contain information that can help us make better decisions in the future. Formally, a counterfactual statement corresponds to updating the noise distributions of an SCM and then performing an intervention.</p> <p>Computing counterfactual statements basically consist of the following steps: (1) modify the original SCM to construct a corresponding counterfactual SCM based on the observations; (2) intervene on variables of the counterfactual SCM; (3) compute the expectation of interested variables. For example, let’s introduce counterfactuals by assuming an SCM \({\boldsymbol{\zeta}} := ({\bf S}, P_{\bf N})\) such that</p> \[X_1:=N_{X_1} \\ X_2:=X_1 +N_{X_2} \\ X_3:=X_2-X_1 + N_{X_3}, \tag {13}\] <p>with \(N_{X_1}, N_{X_2}, N_{X_3} \mathop{\sim}^{iid}U{({1,2,...,4,5})}\).</p> <p>If we have an observation \(\mathbf{X = x}\) namely \((X_1, X_2, X_3) = (1, 2, 4)\), then we get \((N_{X_1}, N_{X_2}, N_{X_3}) = (1, 1, 3)\). From this observation, we can define a new SCM \(\boldsymbol{\zeta}^{\bf X=x} := (\mathbf{S}, P^{\boldsymbol{\zeta}\lvert {\bf X= x}}_\mathbf{N})\). The new SCM is a counterfactual SCM related to the original one. Then, counterfactual statements can be seen by intervening statements in the counterfactual SCM, such as “\(X_2\) would have been 3, had \(X_1\) been 2”, “\(X_3\) would have been 5, had \(X_2\) been 3”. However, we can not have a statement such that “\(X_3\) would have been 5, had \(X_1\) been 2”, which is because counterfactual statements are not transitive. In this case, the correct statement is “\(X_3\) would have been 4, had \(X_1\) been 2”, of which the reason is the existence of a direct link from \(X_1\) to \(X_3\) that does not pass \(X_2\). The above process can be used to compute deterministic counterfactuals. But counterfactuals can also be probabilistic, pertaining to a class of units within the population. To compute probabilistic counterfactuals, the general process is analogous: First, update the noise distributions based on the observations to construct a new SCM; then, counterfactuals can be seen from intervention statements from the newly constructed SCM.</p> <p>With regard to practical uses of counterfactuals, there is an interesting example in <span style="color:purple">[2]</span>: There is a training program for unemployed people, the government wants to prove whether it helps them get hired. Letting \(T = 1\) represent training and \(R = 1\) represent hiring, a pilot randomized experiment shows that the average causal effect (ACE) is positive such that</p> \[ACE = \mathbb{E}[R^{do (T:=1)}] - \mathbb{E}[R^{do (T:=0)}] &gt; 0 \tag {14}\] <p>Then, the government opens up the program to all unemployed people who want to get hired, and the hiring rate among the program’s graduates encouragingly turns out even higher than in the randomized pilot experiment. Although the program is somewhat effective in the experiment study, critics claim that there is no evidence that the program contributes to getting rehired for those who choose the training project of their own volition. It may be that people who actively participated in that training program have other unobserved advantages, such as being more proactive and sociable than people who did not. The critics claim what we need to estimate is the effect of treatment on the treated (ETT)</p> \[ETT = \mathbb{E}[R^{do(T:=1)} - R^{do(T:=0)}\lvert T=1], \tag {15}\] <p>which means the extent to which the hiring rate has increased among the enrolled, compared to what it would have been had they not been trained. At first sight, the expression for ETT does not appear to be estimable from either observational or experimental data. However, ETT can be identifiable in many cases. For example, one such situation occurs when a set \(Z\) of covariates satisfies the backdoor criterion with regard to the treatment and outcome variables.</p> <p><strong>Reference</strong></p> <p>[1] Jonas Peters, Dominik Janzing, and Bernhard Sch¨olkopf. Elements of Causal Inference: Foundations and Learning Algorithms. MIT Press, Cambridge, MA, 2017.</p> <p>[2] Judea Pearl, Madelyn Glymour, and Nicholas P. Jewell. Counterfactuals and Their Applications. In Causal Inference in Statistics: A Primer. John Wiley &amp; Sons, Ltd, 2016.</p> <p>[3] d-separation: How to determine which variables are independent in a Bayes net. <a href="http://web.mit.edu/jmn/www/6.034/d-separation.pdf">Available online</a>, (accessed on Nov. 3, 2021).</p> <p>[4] Peter Spirtes and Kun Zhang. <a href="https://doi.org/10.1186/s40535-016-0018-x">Causal discovery and inference: concepts and recent methodological advances.</a> Appl Inform 3, 3 (2016).</p>]]></content><author><name></name></author><category term="reading"/><category term="study"/><summary type="html"><![CDATA[A reading report of "Elements of causal inference" by J. Peters et al.]]></summary></entry></feed>